{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92666be9-d463-4837-ab27-613f9a82ce37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3358ee-14d5-40c9-a946-52ef1706ee98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Using cached gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/vitaminx/anaconda3/envs/ml/lib/python3.7/site-packages (from gymnasium) (6.7.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/vitaminx/anaconda3/envs/ml/lib/python3.7/site-packages (from gymnasium) (2.2.1)\n",
      "Collecting farama-notifications>=0.0.1\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Collecting jax-jumpy>=1.0.0\n",
      "  Using cached jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/vitaminx/anaconda3/envs/ml/lib/python3.7/site-packages (from gymnasium) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/vitaminx/anaconda3/envs/ml/lib/python3.7/site-packages (from gymnasium) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vitaminx/anaconda3/envs/ml/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.15.0)\n",
      "Installing collected packages: farama-notifications, jax-jumpy, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56be5fe1-e2f4-47eb-81d3-b34f6d234125",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "2025-07-30 15:07:43.356255: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/vitaminx/anaconda3/envs/ml/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary moduels \n",
    "import gym\n",
    "import random\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import models, layers \n",
    "from collections import deque \n",
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "\n",
    "# Set random seed s for reproducibility \n",
    "np.random.seed(42) \n",
    "tf.random.set_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854d1869-e9d2-4bec-bb34-d59a037cd46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Hyperparameters \n",
    "gamma = 0.99 \n",
    "learningRate = 0.001\n",
    "memorySize = 10000\n",
    "batchSize = 64 \n",
    "epsilon = 1.0 \n",
    "epsilonMin = 0.01\n",
    "epsilonDecay = 0.995 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a6540-381b-4ed3-b88d-092ae150f99e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the DQN class \n",
    "class DQN:\n",
    "    def __init__(self, stateSize, actionSize):\n",
    "        self.stateSize = stateSize\n",
    "        self.actionSize = actionSize\n",
    "        self.memory = deque(maxlen=memorySize)\n",
    "        self.epsilon = epsilon\n",
    "        self.model = self._buildModel()\n",
    "        self.targetModel = self._buildModel()\n",
    "        self.updateTargetModel()\n",
    "        \n",
    "    def save(self, filePath):\n",
    "        self.model.save(filePath)\n",
    "\n",
    "    def load(self, filePath):\n",
    "        self.model = tf.keras.models.load_model(filePath)\n",
    "        self.updateTargetModel()\n",
    "\n",
    "    def _buildModel(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(24, input_dim=self.stateSize, activation='relu'),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(self.actionSize, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate))\n",
    "        return model\n",
    "\n",
    "    def updateTargetModel(self):\n",
    "        self.targetModel.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, nextState, done):\n",
    "        self.memory.append((state, action, reward, nextState, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.actionSize)\n",
    "        actValues = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(actValues[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < batchSize:\n",
    "            return\n",
    "        miniBatch = random.sample(self.memory, batchSize)\n",
    "        states = np.array([t[0][0] for t in miniBatch])\n",
    "        actions = np.array([t[1] for t in miniBatch])\n",
    "        rewards = np.array([t[2] for t in miniBatch])\n",
    "        nextStates = np.array([t[3][0] for t in miniBatch])\n",
    "        dones = np.array([t[4] for t in miniBatch])\n",
    "\n",
    "        targets = self.model.predict(states, verbose=0)\n",
    "        targetsNext = self.targetModel.predict(nextStates, verbose=0)\n",
    "\n",
    "        for i in range(batchSize):\n",
    "            if dones[i]:\n",
    "                targets[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                targets[i][actions[i]] = rewards[i] + gamma * np.amax(targetsNext[i])\n",
    "\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > epsilonMin:\n",
    "            self.epsilon *= epsilonDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034fde6e-2683-4894-ba44-9e2ffd1d9470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def trainDqn(episodes, renderMode='none', displayFreq=10):\n",
    "    # Initialize environment with appropriate render mode\n",
    "    if renderMode == 'human':\n",
    "        cartPoleEnv = gym.make('CartPole-v1', render_mode='human')\n",
    "    else:\n",
    "        cartPoleEnv = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "    stateSize = cartPoleEnv.observation_space.shape[0]\n",
    "    actionSize = cartPoleEnv.action_space.n\n",
    "    dqnAgent = DQN(stateSize, actionSize)\n",
    "    episodeScores = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        resetResult = cartPoleEnv.reset()\n",
    "        currentState = resetResult[0] if isinstance(resetResult, tuple) else resetResult\n",
    "        currentState = np.reshape(currentState, [1, stateSize])\n",
    "        episodeScore = 0\n",
    "\n",
    "        for step in range(500):  # Max steps per episode\n",
    "            if renderMode != 'none' and (episode + 1) % displayFreq == 0:\n",
    "                if renderMode == 'human':\n",
    "                    cartPoleEnv.render()\n",
    "                else:\n",
    "                    frame = cartPoleEnv.render()\n",
    "                    if frame is not None:\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                        frame = cv2.resize(frame, (400, 300))\n",
    "                        clear_output(wait=True)\n",
    "                        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "            action = dqnAgent.act(currentState)\n",
    "            stepResult = cartPoleEnv.step(action)\n",
    "\n",
    "            nextState = stepResult[0]\n",
    "            reward = stepResult[1]\n",
    "            terminated = stepResult[2]\n",
    "            truncated = stepResult[3]\n",
    "            done = terminated or truncated\n",
    "\n",
    "            nextState = np.reshape(nextState, [1, stateSize])\n",
    "            dqnAgent.remember(currentState, action, reward, nextState, done)\n",
    "            currentState = nextState\n",
    "            episodeScore += reward\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode: {episode + 1}/{episodes}, Score: {episodeScore}, Epsilon: {dqnAgent.epsilon:.2f}\")\n",
    "                episodeScores.append(episodeScore)\n",
    "                break\n",
    "\n",
    "            dqnAgent.replay()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            dqnAgent.updateTargetModel()\n",
    "\n",
    "    cartPoleEnv.close()\n",
    "    return episodeScores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ad5cca-8ade-4263-996c-01930f520868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Cell 5: Train and Plot\n",
    "# episodes = 100\n",
    "\n",
    "# # Choose renderMode: 'human', 'notebook', or 'none'\n",
    "# scores = trainDqn(episodes, renderMode='notebook', displayFreq=10)\n",
    "\n",
    "# # Plot the results\n",
    "# plt.plot(scores)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Score')\n",
    "# plt.title('Training Progress')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171d208-64c1-4262-a379-9f26f876da51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
